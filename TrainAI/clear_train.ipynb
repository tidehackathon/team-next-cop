{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "DRON_PATH = \"./dron\"\n",
    "MAP_MOLES_PATH = \"./map\"\n",
    "IMAGE_SIZE = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(file_pass, array_name, array_label):\n",
    "  for root, dirs, files in os.walk(file_pass): \n",
    "    for i, j in enumerate(files):\n",
    "      if(\".png\" in j):       \n",
    "        img = cv2.imread(f\"{file_pass}/{j}\")\n",
    "\n",
    "        if img is None:\n",
    "            print('Wrong path:', f\"{file_pass}/{j}\")\n",
    "        else:\n",
    "            array_name.append(tf.image.resize(img, (IMAGE_SIZE, IMAGE_SIZE)))\n",
    "            # adding a photo label depending on the folder\n",
    "            if root == DRON_PATH:\n",
    "              label_array.append(1)\n",
    "            else:\n",
    "              label_array.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download train set\n",
    "all_im_array_dron = []\n",
    "label_array_dron = []\n",
    "\n",
    "cycle(DRON_PATH, all_im_array, label_array)\n",
    "cycle(MAP_MOLES_PATH, all_im_array, label_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_im_array))\n",
    "print(len(label_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train = np.array(all_im_array[:1600])\n",
    "labels_train = np.array(label_array[:1600])\n",
    "\n",
    "validation_images = np.array(all_im_array[1600:])\n",
    "validation_labels = np.array(label_array[1600:])\n",
    "validation_images.shape, validation_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_model = tf.keras.models.load_model(\"flower_model_bit_0.96875\")\n",
    "bit_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Input((IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "        tf.keras.layers.Rescaling(scale=1.0 / 255),\n",
    "        bit_model.layers[1],\n",
    "        tf.keras.layers.Normalization(mean=0, variance=1),\n",
    "    ],\n",
    "    name=\"embedding_model\",\n",
    ")\n",
    "\n",
    "embedding_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_func(embedding, random_vectors):\n",
    "    embedding = np.array(embedding)\n",
    "\n",
    "    # Random projection.\n",
    "    bools = np.dot(embedding, random_vectors) > 0\n",
    "    return [bool2int(bool_vec) for bool_vec in bools]\n",
    "\n",
    "\n",
    "def bool2int(x):\n",
    "    y = 0\n",
    "    for i, j in enumerate(x):\n",
    "        if j:\n",
    "            y += 1 << i\n",
    "    return y\n",
    "\n",
    "\n",
    "class Table:\n",
    "    def __init__(self, hash_size, dim):\n",
    "        self.table = {}\n",
    "        self.hash_size = hash_size\n",
    "        self.random_vectors = np.random.randn(hash_size, dim).T\n",
    "\n",
    "    def add(self, id, vectors, label):\n",
    "        # Create a unique indentifier.\n",
    "        entry = {\"id_label\": str(id) + \"_\" + str(label)}\n",
    "\n",
    "        # Compute the hash values.\n",
    "        hashes = hash_func(vectors, self.random_vectors)\n",
    "\n",
    "        # Add the hash values to the current table.\n",
    "        for h in hashes:\n",
    "            if h in self.table:\n",
    "                self.table[h].append(entry)\n",
    "            else:\n",
    "                self.table[h] = [entry]\n",
    "\n",
    "    def query(self, vectors):\n",
    "        # Compute hash value for the query vector.\n",
    "        hashes = hash_func(vectors, self.random_vectors)\n",
    "        results = []\n",
    "\n",
    "        # Loop over the query hashes and determine if they exist in\n",
    "        # the current table.\n",
    "        for h in hashes:\n",
    "            if h in self.table:\n",
    "                results.extend(self.table[h])\n",
    "        return results\n",
    "    \n",
    "    \n",
    "class LSH:\n",
    "    def __init__(self, hash_size, dim, num_tables):\n",
    "        self.num_tables = num_tables\n",
    "        self.tables = []\n",
    "        for i in range(self.num_tables):\n",
    "            self.tables.append(Table(hash_size, dim))\n",
    "\n",
    "    def add(self, id, vectors, label):\n",
    "        for table in self.tables:\n",
    "            table.add(id, vectors, label)\n",
    "\n",
    "    def query(self, vectors):\n",
    "        results = []\n",
    "        for table in self.tables:\n",
    "            results.extend(table.query(vectors))\n",
    "        return results\n",
    "    \n",
    "    \n",
    "class BuildLSHTable:\n",
    "    def __init__(\n",
    "        self,\n",
    "        prediction_model,\n",
    "        concrete_function=False,\n",
    "        hash_size=8,\n",
    "        dim=2048,\n",
    "        num_tables=10,\n",
    "    ):\n",
    "        self.hash_size = hash_size\n",
    "        self.dim = dim\n",
    "        self.num_tables = num_tables\n",
    "        self.lsh = LSH(self.hash_size, self.dim, self.num_tables)\n",
    "\n",
    "        self.prediction_model = prediction_model\n",
    "        self.concrete_function = concrete_function\n",
    "\n",
    "    def train(self, training_files):\n",
    "        for id, training_file in enumerate(training_files):\n",
    "            # Unpack the data.\n",
    "            image, label = training_file\n",
    "            if len(image.shape) < 4:\n",
    "                image = image[None, ...]\n",
    "\n",
    "            # Compute embeddings and update the LSH tables.\n",
    "            # More on `self.concrete_function()` later.\n",
    "            if self.concrete_function:\n",
    "                features = self.prediction_model(tf.constant(image))[\n",
    "                    \"normalization\"\n",
    "                ].numpy()\n",
    "            else:\n",
    "                features = self.prediction_model.predict(image)\n",
    "            self.lsh.add(id, features, label)\n",
    "\n",
    "    def query(self, image, verbose=True):\n",
    "        # Compute the embeddings of the query image and fetch the results.\n",
    "        if len(image.shape) < 4:\n",
    "            image = image[None, ...]\n",
    "\n",
    "        if self.concrete_function:\n",
    "            features = self.prediction_model(tf.constant(image))[\n",
    "                \"normalization\"\n",
    "            ].numpy()\n",
    "        else:\n",
    "            features = self.prediction_model.predict(image)\n",
    "\n",
    "        results = self.lsh.query(features)\n",
    "        if verbose:\n",
    "            print(\"Matches:\", len(results))\n",
    "\n",
    "        # Calculate Jaccard index to quantify the similarity.\n",
    "        counts = {}\n",
    "        for r in results:\n",
    "            if r[\"id_label\"] in counts:\n",
    "                counts[r[\"id_label\"]] += 1\n",
    "            else:\n",
    "                counts[r[\"id_label\"]] = 1\n",
    "        for k in counts:\n",
    "            counts[k] = float(counts[k]) / self.dim\n",
    "        return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to warm up the GPU.\n",
    "def warmup():\n",
    "    dummy_sample = tf.ones((1, IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    for _ in range(100):\n",
    "        _ = embedding_model.predict(dummy_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = zip(images_train, labels_train)\n",
    "lsh_builder = BuildLSHTable(embedding_model)\n",
    "lsh_builder.train(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, labels):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    columns = 7\n",
    "    for (i, image) in enumerate(images):\n",
    "        ax = plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Query Image\\n\" + \"Label: {}\".format(labels[i]))\n",
    "        else:\n",
    "            ax.set_title(\"Similar Image # \" + str(i) + \"\\nLabel: {}\".format(labels[i]))\n",
    "        plt.imshow(image.astype(\"int\"))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "\n",
    "def visualize_lsh(lsh_class):\n",
    "    idx = np.random.choice(len(validation_images))\n",
    "    image = validation_images[idx]\n",
    "    label = validation_labels[idx]\n",
    "    results = lsh_class.query(image)\n",
    "\n",
    "    candidates = []\n",
    "    labels = []\n",
    "    overlaps = []\n",
    "\n",
    "    for idx, r in enumerate(sorted(results, key=results.get, reverse=True)):\n",
    "        if idx == 4:\n",
    "            break\n",
    "        image_id, label = r.split(\"_\")[0], r.split(\"_\")[1]\n",
    "        candidates.append(images_train[int(image_id)])\n",
    "        labels.append(label)\n",
    "        overlaps.append(results[r])\n",
    "\n",
    "    candidates.insert(0, image)\n",
    "    labels.insert(0, label)\n",
    "    for i in labels:\n",
    "        print(type(i))\n",
    "    plot_images(candidates, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(7):\n",
    "    visualize_lsh(lsh_builder)\n",
    "\n",
    "#visualize_lsh(lsh_builder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0aa97553a034abc1eb8e62d977046d95bed237a67c9d4615d19896c09cb2ba5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
